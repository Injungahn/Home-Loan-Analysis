{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nimport sklearn.model_selection as ms\nfrom sklearn import linear_model\nimport sklearn.metrics as sklm\nfrom time import time",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Read in data\nt0 = time()\ndata = pd.read_csv('train_values_prepped.csv')\nlabels = pd.read_csv('train_labels_prepped.csv')\n",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "numerical = ['loan_amount']\nnum = data[numerical]\n",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Impute missing values for loan_amount\nfrom sklearn.preprocessing import Imputer\nimp = Imputer(missing_values = np.nan, strategy = 'most_frequent')\nimp = imp.fit(num)\nnum_imputed = imp.transform(num)\nnum_imputed = pd.DataFrame(data=num_imputed, columns=numerical)\nnum = num_imputed\n",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Scales the data. Makes individual scalers for each numerical column\nfrom sklearn.preprocessing import MinMaxScaler\nscalers = {}\nfor col in num.columns:\n    scaler = MinMaxScaler()\n    temp = num[col]\n    temp = np.array(temp)\n    temp = temp.reshape(-1, 1)\n    scaler.fit(temp)\n    num_scaled = scaler.transform(temp)\n    scalers[col] = scaler\n    num[col] = num_scaled\n    \nnum_scaled = num\nprint(num_scaled.mean(axis=0))\nprint(num_scaled.std(axis=0))",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "loan_amount    0.345986\ndtype: float64\nloan_amount    0.212585\ndtype: float64\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Encodes the categorical features puts them into cat_features\n\ndata['property_type'][0] = 3\n\n\ncat = ['property_type','loan_purpose','occupancy','preapproval','applicant_ethnicity','applicant_race','applicant_sex',\n      'co_applicant']\n\ndef encode_string(cat_feature):\n    ## First encode the strings to numeric categories\n    enc = preprocessing.LabelEncoder()\n    enc.fit(cat_feature)\n    enc_cat_feature = enc.transform(cat_feature)\n    ## Now, apply one hot encoding\n    ohe = preprocessing.OneHotEncoder()\n    encoded = ohe.fit(enc_cat_feature.reshape(-1,1))\n    return encoded.transform(enc_cat_feature.reshape(-1,1)).toarray()\n    \ncat_features = encode_string(data['loan_type'])\nfor col in cat:\n    temp = encode_string(data[col])\n    cat_features = np.concatenate([cat_features, temp], axis = 1)\n    print(str(col) + \" \" + str(temp.shape))\n\ndata['property_type'][0] = 1\n\n    ",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda2_501/lib/python2.7/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  app.launch_new_instance()\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "property_type (191426, 3)\nloan_purpose (191426, 3)\noccupancy (191426, 3)\npreapproval (191426, 3)\napplicant_ethnicity (191426, 4)\napplicant_race (191426, 7)\napplicant_sex (191426, 4)\nco_applicant (191426, 2)\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda2_501/lib/python2.7/site-packages/ipykernel/__main__.py:25: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Concats numerical scaled features and cat_features. Then uses kbest to select the best 16\nall_features = np.concatenate([cat_features, num_scaled], axis = 1)\n\ny = labels['rate_spread']\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nbestfeatures = SelectKBest(score_func=chi2, k=15)\nfit = bestfeatures.fit(all_features, y)\ndfscores = pd.DataFrame(fit.scores_)\n# dfcolumns = pd.DataFrame(numerical)\nfeatureScores = pd.concat([dfscores], axis=1)\nfeatureScores.columns = ['Score']\nfeatureScores = featureScores.round(3)\nprint(featureScores)\nprint(featureScores.nlargest(15,'Score'))  #print 10 best features\nFeatures_reduced = fit.transform(all_features)\nprint(Features_reduced.shape)",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": "        Score\n0   25023.661\n1   19447.141\n2     239.789\n3     860.994\n4    9267.669\n5   48731.214\n6     279.382\n7    1772.746\n8   12772.889\n9    2102.274\n10     27.762\n11    404.318\n12     86.377\n13   1264.106\n14   6718.921\n15   2547.010\n16   1045.627\n17    534.360\n18   8261.287\n19     61.903\n20    302.082\n21    114.095\n22    404.256\n23     32.172\n24    144.379\n25   1398.455\n26     62.109\n27     64.268\n28    103.877\n29    801.604\n30    112.995\n31    422.173\n32    697.318\n33   4500.966\n        Score\n5   48731.214\n0   25023.661\n1   19447.141\n8   12772.889\n4    9267.669\n18   8261.287\n14   6718.921\n33   4500.966\n15   2547.010\n9    2102.274\n7    1772.746\n25   1398.455\n13   1264.106\n16   1045.627\n3     860.994\n(191426, 15)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Reduces features using the SelectKBest algorithm to 15\nFeatures_reduced = fit.transform(all_features)\nprint(Features_reduced.shape)",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": "(191426, 15)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Splits data into train and test data\nimport numpy.random as nr\n\nnr.seed(9988)\nlabels = y\nindx = range(Features_reduced.shape[0])\nindx = ms.train_test_split(indx, test_size = 40000)\nx_train = Features_reduced[indx[0],:]\ny_train = np.ravel(labels[indx[0]])\nx_test = Features_reduced[indx[1],:]\ny_test = np.ravel(labels[indx[1]])\n\n\n",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Creates a DecisionTreeRegressor model fits \nfrom sklearn import linear_model\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn import tree\nfrom sklearn import svm\n\n# lin_mod = linear_model.LinearRegression(fit_intercept = False)\n# lin_mod = tree.DecisionTreeRegressor(min_samples_split=500)\n# lin_mod = GradientBoostingRegressor()\n# lin_mod = linear_model.Ridge(alpha=.5)\n# lin_mod = linear_model.BayesianRidge()\n# x_train = x_train[:len(x_train)/10]\n# y_train = y_train[:len(y_train)/10]\n# lin_mod = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0])\nlin_mod = svm.SVR()\nlin_mod.fit(x_train, y_train)\n",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda2_501/lib/python2.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n  from numpy.core.umath_tests import inner1d\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Prints the r2 values\nimport math\n\ndef print_metrics(y_true, y_predicted, n_parameters):\n    ## First compute R^2 and the adjusted R^2\n    r2 = sklm.r2_score(y_true, y_predicted)\n    r2_adj = r2 - (n_parameters - 1)/(y_true.shape[0] - n_parameters) * (1 - r2)\n    \n    ## Print the usual metrics and the R^2 values\n    print('Mean Square Error      = ' + str(sklm.mean_squared_error(y_true, y_predicted)))\n    print('Root Mean Square Error = ' + str(math.sqrt(sklm.mean_squared_error(y_true, y_predicted))))\n    print('Mean Absolute Error    = ' + str(sklm.mean_absolute_error(y_true, y_predicted)))\n    print('Median Absolute Error  = ' + str(sklm.median_absolute_error(y_true, y_predicted)))\n    print('R^2                    = ' + str(r2))\n    print('Adjusted R^2           = ' + str(r2_adj))\n   \ny_score = lin_mod.predict(x_test) \nprint_metrics(y_test, y_score, 28)    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def hist_resids(y_test, y_score):\n    ## first compute vector of residuals. \n    resids = np.subtract(y_test.reshape(-1,1), y_score.reshape(-1,1))\n    ## now make the residual plots\n    sns.distplot(resids)\n    plt.title('Histogram of residuals')\n    plt.xlabel('Residual value')\n    plt.ylabel('count')\n    \nhist_resids(y_test, y_score)    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import scipy.stats as ss\ndef resid_qq(y_test, y_score):\n    ## first compute vector of residuals. \n    resids = np.subtract(y_test.reshape(-1,1), y_score.reshape(-1,1))\n    ## now make the residual plots\n    ss.probplot(resids.flatten(), plot = plt)\n    plt.title('Residuals vs. predicted values')\n    plt.xlabel('Predicted values')\n    plt.ylabel('Residual')\n    \nresid_qq(y_test, y_score)   ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "def resid_plot(y_test, y_score):\n    ## first compute vector of residuals. \n    resids = np.subtract(y_test.reshape(-1,1), y_score.reshape(-1,1))\n    ## now make the residual plots\n    sns.regplot(y_score, resids, fit_reg=False)\n    plt.title('Residuals vs. predicted values')\n    plt.xlabel('Predicted values')\n    plt.ylabel('Residual')\n\nresid_plot(y_test, y_score) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "y_score_untransform = np.exp(y_score)\ny_test_untransform = np.exp(y_test)\nresid_plot(y_test_untransform, y_score_untransform) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false
      },
      "cell_type": "code",
      "source": "def plot_scatter(data, col_x, col_y):\n    fig = plt.figure(figsize=(7,6))\n    ax = fig.gca()\n    data.plot.scatter(x = col_x, y = col_y, ax = ax)\n    ax.set_title('Scatter plot of ' + 'predicted' + ' vs. ' + 'actual')\n    ax.set_ylabel('predicted')\n    ax.set_xlabel('actual')\n\n    plt.plot(range(15))\n    plt.xlim(0, 15)\n    plt.ylim(0, 15)\n    plt.gca().set_aspect('equal', adjustable='box')\n    plt.draw()\n\n    plt.show()\n\ndf = pd.DataFrame(y_test)\ndf['y_score'] = y_score\ndf['y_test'] = y_test\n\nplot_scatter(df,'y_score','y_test')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Gets Test values and scales the numerical columns\ntest = pd.read_csv('test_values.csv')\nprint(test.isna().sum())\n\ntest_num = test[numerical]\nfor col in test_num.columns:\n    null_values = test_num[col].isnull()\n    test_num.loc[~null_values, [col]] = scalers[col].transform(test_num.loc[~null_values, [col]] )\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Imputes values for categorical variables and creates categorical features\ntest_cat_features = encode_string(test['loan_type'])\nfor col in cat:\n    temp = encode_string(test[col])\n    test_cat_features = np.concatenate([test_cat_features, temp], axis = 1)\n    print(str(col) + \" \" + str(temp.shape))\n\n    \n# test_all_features = np.concatenate([test_cat_features,test_num_scaled],axis = 1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "#imputes for numerical variables\ntest_num_scaled = test_num\nfrom sklearn.preprocessing import Imputer\nimp = Imputer(missing_values = np.nan, strategy = 'mean')\nimp = imp.fit(test_num_scaled)\ntest_num_scaled = imp.transform(test_num_scaled)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "test_all_features = np.concatenate([test_cat_features,test_num_scaled],axis = 1)\nprint(test_all_features.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "test_all_features = np.concatenate([test_cat_features,test_num_scaled],axis = 1)\nTest_Features_reduced = fit.transform(test_all_features)\nprint(Test_Features_reduced.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "#Makes predictions and outputs it into a csv file\n\n# Test_Features_reduced = sel.fit_transform(Test_Features_reduced)\n\npredictions = lin_mod.predict(Test_Features_reduced) \n\ndf = pd.DataFrame(predictions)\nids = pd.DataFrame(test['row_id'])\nids['rate_spread'] = df\nids.to_csv('predictions.csv', index = None, header=True)\nprint(\"training time:\", round(time() - t0, 3))\n",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.15",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 2,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}